---
title: 图像分割学习总结
date: 2019-04-25 21:01:07
tags: [deeplearn, 总结, 训练技巧]
categories: [deeplearn]
---

记录一下连续写了两个肿瘤分割（图像分割）的感觉。与一路走过的坑。

## model

### fcn

用vgg16的权重，省去训练前几层的麻烦，但是模型很大，如果真的要训练前几层，没有个8g显存别想着训练。

### unet

模型很大，让我很头疼，950显卡batch_size=1也放不下，没办法，借用的1080 loss还不下降。除了最后一层，剩下的卷积层后都加上Batch Normalization，很有用。效果挺好的一个模型，相对新模型来说，采用的还是堆积式，深度相对较小，模型相对较大。

### densenet fcn

模型相对挺小，至少950显卡可以训练。采用的是反卷积而不是upsample。

### enet

模型相对挺小，效果很挺不错。

## loss

### loss函数

loss函数的定义，对结果有着很大的影响。会影响的学习方向

常见loss函数

* dice_loss (1 - dice) 
* 加权loss  一般mask图中，都是1少0多

使用diceloss比较好，加权的比例不好控制，diceloss关注于标注部分，比较好。

### loss不下降

记录下解决方案。

* 加Batch Normalization, 加Batch Normalization很管用，尤其是数据没有归一化的时候。
* 数据归一化。
* 调整模型，和图像大小相等的featuremap最好不用
* 优化函数，学习率。会影响一点
* 随机乱序，在最后抖动的时候，会很稳定

## 数据预处理


### 归一化

归一化对模型的影响，真的是很大。

### 数据加强

可以扩充数据

### 随机乱序

batch——size小，并且没有随机乱序的话，模型会学习到一个局部的数据，模型鲁棒性低。随机乱序对模型有很大的作用。
